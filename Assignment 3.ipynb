{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3\n",
    "Goal: classify [Yelp reviews](https://www.yelp.com/dataset) tagged as funny by at least one user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, Convolution1D, Flatten, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import TensorBoard\n",
    "import pymongo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data \n",
    "The yelp data was imported into a collection called 'reviews' in a MongoDB database called 'yelp'. We pulled a random sample of 80000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient('mongodb://localhost:27017/')\n",
    "db = client['yelp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_docs = 80000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_funny = db.reviews.aggregate([\n",
    "    { '$match': {\n",
    "            'funny': {\n",
    "                '$exists': True } } }, \n",
    "    { '$sample': { \n",
    "            'size': num_docs } }, \n",
    "    { '$project': {\n",
    "            '_id': 0, \n",
    "            'funny': 1, \n",
    "            'text': 1 } }\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_x = []\n",
    "yelp_y = []\n",
    "for review in yelp_funny:\n",
    "    yelp_x.append(review['text'])\n",
    "    yelp_y.append(1 if review['funny'] > 0 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is not very balanced. Only about 20% are funny."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16587"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_funny = np.array(yelp_y).sum()\n",
    "num_funny"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete some not funny reviews to balance the categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_funny_indexes = [i for i, j in enumerate(yelp_y) if j == 0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46826"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_funny_to_delete = not_funny_indexes[num_funny:]\n",
    "len(not_funny_to_delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_funny_to_delete.sort(reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in not_funny_to_delete:\n",
    "    del yelp_x[a]\n",
    "    del yelp_y[a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33174 33174\n"
     ]
    }
   ],
   "source": [
    "print(\"{} {}\".format(len(yelp_x), len(yelp_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle the lists so that there aren't too many funny ones in test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "c = list(zip(yelp_x, yelp_y))\n",
    "random.shuffle(c)\n",
    "yelp_x, yelp_y = zip(*c)\n",
    "yelp_x = list(yelp_x)\n",
    "yelp_y = list(yelp_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tokenizer(top_words)\n",
    "foo = t.fit_on_texts(yelp_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "998"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length = max([len(s.split()) for s in yelp_x])\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = t.texts_to_sequences(yelp_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sequence.pad_sequences(X, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26539"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split = round(len(X) *.8)\n",
    "split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[:split - 1]\n",
    "X_test = X[split:]\n",
    "Y_train = yelp_y[:split -1]\n",
    "Y_test = yelp_y[split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1b: CNN for sentence classification\n",
    "Adapted from https://github.com/Theo-/sentiment-analysis-keras-conv/blob/master/train_keras.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 998, 300)          3000000   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 998, 64)           57664     \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 998, 32)           6176      \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 998, 16)           1552      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 15968)             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 15968)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 180)               2874420   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 180)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 181       \n",
      "=================================================================\n",
      "Total params: 5,939,993\n",
      "Trainable params: 5,939,993\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Using embedding from Keras\n",
    "embedding_vecor_length = 300\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_length))\n",
    "\n",
    "# Convolutional model (3x conv, flatten, 2x dense)\n",
    "model.add(Convolution1D(64, 3, padding='same'))\n",
    "model.add(Convolution1D(32, 3, padding='same'))\n",
    "model.add(Convolution1D(16, 3, padding='same'))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(180,activation='sigmoid'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "# Log to tensorboard\n",
    "tensorBoardCallback = TensorBoard(log_dir='./logs', write_graph=True)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "26538/26538 [==============================] - 206s 8ms/step - loss: 0.6582 - acc: 0.6287\n",
      "Epoch 2/3\n",
      "26538/26538 [==============================] - 206s 8ms/step - loss: 0.5172 - acc: 0.7439\n",
      "Epoch 3/3\n",
      "26538/26538 [==============================] - 214s 8ms/step - loss: 0.2024 - acc: 0.9160\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11a3ea390>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, epochs=3, callbacks=[tensorBoardCallback], batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6635/6635 [==============================] - 17s 3ms/step\n",
      "Accuracy: 61.63%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on the test set\n",
    "scores = model.evaluate(X_test, Y_test, verbose=1)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2b: RNN for sentence classification\n",
    "Adapted from https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/\n",
    "\n",
    "Goal: classify [Yelp reviews](https://www.yelp.com/dataset) tagged as funny by at least one user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 998, 32)           320000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 373,301\n",
      "Trainable params: 373,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# LSTM with dropout for sequence classification\n",
    "\n",
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_length))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "tensorBoardCallback = TensorBoard(log_dir='./logs', write_graph=True)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "26538/26538 [==============================] - 465s 18ms/step - loss: 0.6934 - acc: 0.5006\n",
      "Epoch 2/3\n",
      "26538/26538 [==============================] - 460s 17ms/step - loss: 0.6933 - acc: 0.5028\n",
      "Epoch 3/3\n",
      "26538/26538 [==============================] - 454s 17ms/step - loss: 0.6933 - acc: 0.4973\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12b708fd0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 50.41%\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1c: Extending CNN with NLP features\n",
    "\n",
    "There are many ways to extend the Yelp review data with NLP features. One example that may help our task would be adding sentime scores to each word, forming tokens like the following: 'good_3', 'bad_1', etc., where each token is paried with a score. We could then feed these modified tokens into the CNN and again classify as funny or not funny. Similarly, we could add other charactersitics like dependency structures, named entity tags, or parts of speech tokens. This last one, using parts of speec tags, is what we do below. Basically, we use NLTL to match every word in our data to a parts of speech tag, and then tokenize the sentences and feed them into the CNN model.\n",
    "\n",
    "\n",
    "Goal: classify [Yelp reviews](https://www.yelp.com/dataset) tagged as funny by at least one user. Apply parts of speech tagging to add additional information, and classidy ussing a convolutional neural net. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_token_pos(tagged):\n",
    "    if (tagged[1] == '.'):\n",
    "        return(tagged[0])\n",
    "    else:\n",
    "        return('{}_{}'.format(tagged[0], tagged[1]))\n",
    "\n",
    "def apply_tags(list_of_sentences):\n",
    "    tagged = []\n",
    "    for sentence in list_of_sentences:\n",
    "        tokenized = nltk.word_tokenize(sentence)\n",
    "        with_pos_tags = [merge_token_pos(tag) for tag in nltk.pos_tag(tokenized)]\n",
    "        tagged.append(\" \".join(with_pos_tags))\n",
    "    return(tagged)\n",
    "        \n",
    "yelp_x_tagged = apply_tags(yelp_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-tagged: 33174\tTagged: 33174\n"
     ]
    }
   ],
   "source": [
    "# Should be the same size\n",
    "print('Non-tagged: {}\\tTagged: {}'.format(len(yelp_x), len(yelp_x_tagged)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"It_PRP would_MD n't_RB be_VB the_DT weekend_NN without_IN Cora_NNP 's_POS . The_DT food_NN is_VBZ always_RB excellent_JJ and_CC the_DT staff_NN are_VBP welcoming_VBG and_CC efficient_JJ . The_DT tough_JJ part_NN is_VBZ deciding_VBG whether_IN to_TO go_VB savory_NN or_CC sweet_NN . The_DT loaded_JJ potatoes_NNS and_CC fresh_JJ fruit_NN are_VBP the_DT perfect_JJ accompaniment_NN .\",\n",
       " \"SW_NNP Eye_NNP Center_NNP is_VBZ terrible_JJ . The_DT people_NNS there_EX are_VBP very_RB rude_JJ and_CC if_IN you_PRP ever_RB have_VBP to_TO have_VB retinal_JJ surgery_NN ,_, avoid_VBP Dr._NNP Adelberg_NNP at_IN all_DT costs_NNS . I_PRP had_VBD retinal_JJ surgery_NN there_RB three_CD years_NNS ago_RB for_IN a_DT macular_JJ hole_NN and_CC I_PRP still_RB do_VBP not_RB have_VB eyesight_VBN in_IN that_DT eye_NN . Other_JJ eye_NN doctors_NNS have_VBP told_VBN me_PRP I_PRP would_MD have_VB had_VBD a_DT much_RB better_RBR result_NN if_IN I_PRP had_VBD used_VBN them_PRP and_CC one_CD described_VBD Adelberg_NNP 's_POS work_NN as_IN ``_`` sloppy_JJ . ''_''\",\n",
       " \"Store_NN review_NN :_: More_JJR grocery_NN options_NNS are_VBP always_RB a_DT plus_NN . I_PRP still_RB ca_MD n't_RB figure_VB out_RP what_WP grocery_NN store_NN will_MD be_VB my_PRP$ go_VB to_TO location_NN in_IN Champaign_NNP . This_DT one_NN has_VBZ some_DT positives_NNS which_WDT include_VBP a_DT free_JJ cookie_NN self_NN serve_NN for_IN kids_NNS under_IN 11_CD with_IN adults_NNS present_JJ . I_PRP was_VBD wondering_VBG why_WRB people_NNS were_VBD just_RB grabbing_VBG a_DT cookie_NN and_CC giving_VBG it_PRP to_TO their_PRP$ kids_NNS to_TO eat_VB . Another_DT nice_JJ positive_JJ is_VBZ 50_CD %_NN off_IN donuts_NNS after_IN 6_CD pm_NN . If_IN you_PRP like_VBP yeast_'' donuts_IN these_DT are_VBP pretty_RB good_JJ . The_DT store_NN is_VBZ clean_JJ and_CC well_RB lit_RB . The_DT only_JJ downside_NN is_VBZ the_DT prices_NNS are_VBP average_JJ or_CC higher_JJR than_IN other_JJ locations_NNS . I_PRP will_MD come_VB here_RB to_TO check_VB it_PRP out_RP for_IN deals_NNS when_WRB I_PRP can_MD . Pros_VB :_: free_JJ bakery_NN cookie_NN for_IN kids_NNS under_IN 11_CD ,_, 50_CD %_NN off_IN donuts_NNS after_IN 6_CD pm_JJ Cons_NNS :_: average_NN prices_NNS Hidden_JJ deals_NNS :_: Free_JJ cookie_NN for_IN kids_NNS 50_CD %_NN off_IN donuts_NNS after_IN 6_CD pm_NN You_PRP can_MD use_VB internet_JJ coupons_NNS ,_, but_CC check_VB out_RP the_DT extensive_JJ coupon_NN policy_NN on_IN the_DT website_NN . No_CC coupons_NNS over_IN 5_CD dollars_NNS ,_, no_DT free_JJ item_NN coupons_NNS ,_, no_DT 2_CD coupons_NNS on_IN BOGO_NNP items_NNS ,_, no_DT coupon_NN value_NN over_IN purchase_NN value_NN\",\n",
       " \"Mediocre_NNP food_NN at_IN best_JJS . Not_RB very_RB flavorful_JJ . I_PRP do_VBP n't_RB think_VB I_PRP would_MD choose_VB to_TO eat_VB here_RB before_IN any_DT other_JJ Mexican_JJ restaurant_NN .\",\n",
       " 'Wow_NNP ,_, this_DT place_NN has_VBZ great_JJ food_NN and_CC service_NN . The_DT wait_JJ staff_NN are_VBP knowledgeable_JJ and_CC super_JJ friendly_RB . The_DT food_NN is_VBZ outstanding_JJ ,_, one_CD of_IN the_DT best_JJS espressos_NN I_PRP found_VBD in_IN Las_NNP Vegas_NNP . Try_VB the_DT chocolate_NN macaroons_NNS it_PRP is_VBZ fantastic_JJ .']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_x_tagged[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = t.texts_to_sequences(yelp_x)\n",
    "X = sequence.pad_sequences(X, maxlen=max_length, padding='post')\n",
    "\n",
    "# Define split\n",
    "split = round(len(X) *.8)\n",
    "split\n",
    "\n",
    "# Actually split the data\n",
    "X_train = X[:split - 1]\n",
    "X_test = X[split:]\n",
    "Y_train = yelp_y[:split -1]\n",
    "Y_test = yelp_y[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 998, 300)          3000000   \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 998, 64)           57664     \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 998, 32)           6176      \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 998, 16)           1552      \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 15968)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 15968)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 180)               2874420   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 180)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 181       \n",
      "=================================================================\n",
      "Total params: 5,939,993\n",
      "Trainable params: 5,939,993\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Using embedding from Keras\n",
    "embedding_vecor_length = 300\n",
    "model_tags = Sequential()\n",
    "model_tags.add(Embedding(top_words, embedding_vecor_length, input_length=max_length))\n",
    "\n",
    "# Convolutional model (3x conv, flatten, 2x dense)\n",
    "model_tags.add(Convolution1D(64, 3, padding='same'))\n",
    "model_tags.add(Convolution1D(32, 3, padding='same'))\n",
    "model_tags.add(Convolution1D(16, 3, padding='same'))\n",
    "model_tags.add(Flatten())\n",
    "model_tags.add(Dropout(0.2))\n",
    "model_tags.add(Dense(180,activation='sigmoid'))\n",
    "model_tags.add(Dropout(0.2))\n",
    "model_tags.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "# Log to tensorboard\n",
    "tensorBoardCallback = TensorBoard(log_dir='./logs', write_graph=True)\n",
    "model_tags.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model_tags.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "26538/26538 [==============================] - 198s 7ms/step - loss: 0.6559 - acc: 0.6254\n",
      "Epoch 2/3\n",
      "26538/26538 [==============================] - 197s 7ms/step - loss: 0.5467 - acc: 0.7242\n",
      "Epoch 3/3\n",
      "26538/26538 [==============================] - 205s 8ms/step - loss: 0.2566 - acc: 0.8920\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x132490c18>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_tags.fit(X_train, Y_train, epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 58.15%\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of the model\n",
    "scores = model_tags.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2c: Extending RNN with NLP features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 998, 32)           320000    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 373,301\n",
      "Trainable params: 373,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# LSTM with dropout for sequence classification\n",
    "\n",
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_length))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "tensorBoardCallback = TensorBoard(log_dir='./logs', write_graph=True)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "26538/26538 [==============================] - 469s 18ms/step - loss: 0.6934 - acc: 0.4972\n",
      "Epoch 2/3\n",
      "26538/26538 [==============================] - 463s 17ms/step - loss: 0.6933 - acc: 0.4970\n",
      "Epoch 3/3\n",
      "26538/26538 [==============================] - 456s 17ms/step - loss: 0.6932 - acc: 0.5046\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12b6c9470>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 49.59%\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
