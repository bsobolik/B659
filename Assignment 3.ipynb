{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3\n",
    "Goal: classify [Yelp reviews](https://www.yelp.com/dataset) tagged as funny by at least one user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, Convolution1D, Flatten, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import TensorBoard\n",
    "import pymongo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data \n",
    "The yelp data was imported into a collection called 'docs' in a MongoDB database called 'yelp'. We pulled a random sample of 10000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient('mongodb://localhost:27017/')\n",
    "db = client['yelp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_docs = 80000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_funny = db.docs.aggregate([\n",
    "    { '$match': {\n",
    "            'funny': {\n",
    "                '$exists': True } } }, \n",
    "    { '$sample': { \n",
    "            'size': num_docs } }, \n",
    "    { '$project': {\n",
    "            '_id': 0, \n",
    "            'funny': 1, \n",
    "            'text': 1 } }\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_x = []\n",
    "yelp_y = []\n",
    "for review in yelp_funny:\n",
    "    yelp_x.append(review['text'])\n",
    "    yelp_y.append(1 if review['funny'] > 0 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is not very balanced. Only about 20% are funny."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16560"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_funny = np.array(yelp_y).sum()\n",
    "num_funny"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete some not funny reviews to balance the categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_funny_indexes = [i for i, j in enumerate(yelp_y) if j == 0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46880"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_funny_to_delete = not_funny_indexes[num_funny:]\n",
    "len(not_funny_to_delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_funny_to_delete.sort(reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in not_funny_to_delete:\n",
    "    del yelp_x[a]\n",
    "    del yelp_y[a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33120 33120\n"
     ]
    }
   ],
   "source": [
    "print(\"{} {}\".format(len(yelp_x), len(yelp_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle the lists so that there aren't too many funny ones in test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "c = list(zip(yelp_x, yelp_y))\n",
    "random.shuffle(c)\n",
    "yelp_x, yelp_y = zip(*c)\n",
    "yelp_x = list(yelp_x)\n",
    "yelp_y = list(yelp_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tokenizer(top_words)\n",
    "foo = t.fit_on_texts(yelp_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1016"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length = max([len(s.split()) for s in yelp_x])\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = t.texts_to_sequences(yelp_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sequence.pad_sequences(X, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26496"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split = round(len(X) *.8)\n",
    "split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[:split - 1]\n",
    "X_test = X[split:]\n",
    "Y_train = yelp_y[:split -1]\n",
    "Y_test = yelp_y[split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1b: CNN for sentence classification\n",
    "Adapted from https://github.com/Theo-/sentiment-analysis-keras-conv/blob/master/train_keras.py and https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1016, 300)         3000000   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 1016, 64)          57664     \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 1016, 32)          6176      \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 1016, 16)          1552      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 16256)             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16256)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 180)               2926260   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 180)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 181       \n",
      "=================================================================\n",
      "Total params: 5,991,833\n",
      "Trainable params: 5,991,833\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Using embedding from Keras\n",
    "embedding_vecor_length = 300\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_length))\n",
    "\n",
    "# Convolutional model (3x conv, flatten, 2x dense)\n",
    "model.add(Convolution1D(64, 3, padding='same'))\n",
    "model.add(Convolution1D(32, 3, padding='same'))\n",
    "model.add(Convolution1D(16, 3, padding='same'))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(180,activation='sigmoid'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "# Log to tensorboard\n",
    "tensorBoardCallback = TensorBoard(log_dir='./logs', write_graph=True)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "26495/26495 [==============================] - 197s 7ms/step - loss: 0.6522 - acc: 0.6245\n",
      "Epoch 2/3\n",
      "26495/26495 [==============================] - 199s 7ms/step - loss: 0.5818 - acc: 0.6963\n",
      "Epoch 3/3\n",
      "26495/26495 [==============================] - 190s 7ms/step - loss: 0.3542 - acc: 0.8440\n",
      "Accuracy: 60.70%\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, epochs=3, callbacks=[tensorBoardCallback], batch_size=64)\n",
    "\n",
    "# Evaluation on the test set\n",
    "scores = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: RNN for sentence classification\n",
    "Adapted from https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/\n",
    "\n",
    "Goal: classify [Yelp reviews](https://www.yelp.com/dataset) tagged as funny by at least one user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 1016, 32)          320000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 373,301\n",
      "Trainable params: 373,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# LSTM with dropout for sequence classification\n",
    "\n",
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_length))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "tensorBoardCallback = TensorBoard(log_dir='./logs', write_graph=True)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "26495/26495 [==============================] - 450s 17ms/step - loss: 0.6933 - acc: 0.5034\n",
      "Epoch 2/3\n",
      "26495/26495 [==============================] - 474s 18ms/step - loss: 0.6933 - acc: 0.4949\n",
      "Epoch 3/3\n",
      "26495/26495 [==============================] - 495s 19ms/step - loss: 0.6933 - acc: 0.4957\n",
      "Accuracy: 50.18%\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, epochs=3, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
