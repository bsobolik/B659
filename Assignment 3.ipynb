{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3\n",
    "Goal: classify [Yelp reviews](https://www.yelp.com/dataset) tagged as funny by at least one user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, Convolution1D, Flatten, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import TensorBoard\n",
    "import pymongo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data \n",
    "The yelp data was imported into a collection called 'docs' in a MongoDB database called 'yelp'. We pulled a random sample of 10000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient('mongodb://localhost:27017/')\n",
    "db = client['yelp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_docs = 80000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_funny = db.reviews.aggregate([\n",
    "    { '$match': {\n",
    "            'funny': {\n",
    "                '$exists': True } } }, \n",
    "    { '$sample': { \n",
    "            'size': num_docs } }, \n",
    "    { '$project': {\n",
    "            '_id': 0, \n",
    "            'funny': 1, \n",
    "            'text': 1 } }\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_x = []\n",
    "yelp_y = []\n",
    "for review in yelp_funny:\n",
    "    yelp_x.append(review['text'])\n",
    "    yelp_y.append(1 if review['funny'] > 0 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is not very balanced. Only about 20% are funny."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16316"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_funny = np.array(yelp_y).sum()\n",
    "num_funny"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete some not funny reviews to balance the categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_funny_indexes = [i for i, j in enumerate(yelp_y) if j == 0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47368"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_funny_to_delete = not_funny_indexes[num_funny:]\n",
    "len(not_funny_to_delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_funny_to_delete.sort(reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in not_funny_to_delete:\n",
    "    del yelp_x[a]\n",
    "    del yelp_y[a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32632 32632\n"
     ]
    }
   ],
   "source": [
    "print(\"{} {}\".format(len(yelp_x), len(yelp_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle the lists so that there aren't too many funny ones in test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "c = list(zip(yelp_x, yelp_y))\n",
    "random.shuffle(c)\n",
    "yelp_x, yelp_y = zip(*c)\n",
    "yelp_x = list(yelp_x)\n",
    "yelp_y = list(yelp_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tokenizer(top_words)\n",
    "foo = t.fit_on_texts(yelp_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1005"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length = max([len(s.split()) for s in yelp_x])\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = t.texts_to_sequences(yelp_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sequence.pad_sequences(X, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26106"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split = round(len(X) *.8)\n",
    "split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[:split - 1]\n",
    "X_test = X[split:]\n",
    "Y_train = yelp_y[:split -1]\n",
    "Y_test = yelp_y[split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1b: CNN for sentence classification\n",
    "Adapted from https://github.com/Theo-/sentiment-analysis-keras-conv/blob/master/train_keras.py and https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/dakotamurray/.pyenv/versions/3.7.1/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/dakotamurray/.pyenv/versions/3.7.1/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1005, 300)         3000000   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 1005, 64)          57664     \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 1005, 32)          6176      \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 1005, 16)          1552      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 16080)             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16080)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 180)               2894580   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 180)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 181       \n",
      "=================================================================\n",
      "Total params: 5,960,153\n",
      "Trainable params: 5,960,153\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Using embedding from Keras\n",
    "embedding_vecor_length = 300\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_length))\n",
    "\n",
    "# Convolutional model (3x conv, flatten, 2x dense)\n",
    "model.add(Convolution1D(64, 3, padding='same'))\n",
    "model.add(Convolution1D(32, 3, padding='same'))\n",
    "model.add(Convolution1D(16, 3, padding='same'))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(180,activation='sigmoid'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "# Log to tensorboard\n",
    "tensorBoardCallback = TensorBoard(log_dir='./logs', write_graph=True)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/dakotamurray/.pyenv/versions/3.7.1/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/3\n",
      "26105/26105 [==============================] - 190s 7ms/step - loss: 0.6551 - acc: 0.6271\n",
      "Epoch 2/3\n",
      "26105/26105 [==============================] - 220s 8ms/step - loss: 0.5649 - acc: 0.7087\n",
      "Epoch 3/3\n",
      "26105/26105 [==============================] - 241s 9ms/step - loss: 0.2976 - acc: 0.8715\n",
      "Accuracy: 61.91%\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, epochs=3, callbacks=[tensorBoardCallback], batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6526/6526 [==============================] - 13s 2ms/step\n",
      "Accuracy: 61.91%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on the test set\n",
    "scores = model.evaluate(X_test, Y_test, verbose=1)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: RNN for sentence classification\n",
    "Adapted from https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/\n",
    "\n",
    "Goal: classify [Yelp reviews](https://www.yelp.com/dataset) tagged as funny by at least one user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 1005, 32)          320000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 373,301\n",
      "Trainable params: 373,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# LSTM with dropout for sequence classification\n",
    "\n",
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_length))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "tensorBoardCallback = TensorBoard(log_dir='./logs', write_graph=True)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "26105/26105 [==============================] - 499s 19ms/step - loss: 0.6934 - acc: 0.5024\n",
      "Epoch 2/3\n",
      "26105/26105 [==============================] - 501s 19ms/step - loss: 0.6932 - acc: 0.5006\n",
      "Epoch 3/3\n",
      "26105/26105 [==============================] - 516s 20ms/step - loss: 0.6933 - acc: 0.4988\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12d014d30>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 49.40%\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.C: Extending CNN with NLP features\n",
    "\n",
    "There are many ways to extend the Yelp review data with NLP features. One example that may help our task would be adding sentime scores to each word, forming tokens like the following: 'good_3', 'bad_1', etc., where each token is paried with a score. We could then feed these modified tokens into the CNN and again classify as funny or not funny. Similarly, we could add other charactersitics like dependency structures, named entity tags, or parts of speech tokens. This last one, using parts of speec tags, is what we do below. Basically, we use NLTL to match every word in our data to a parts of speech tag, and then tokenize the sentences and feed them into the CNN model.\n",
    "\n",
    "\n",
    "Goal: classify [Yelp reviews](https://www.yelp.com/dataset) tagged as funny by at least one user. Apply parts of speech tagging to add additional information, and classidy ussing a convolutional neural net. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_token_pos(tagged):\n",
    "    if (tagged[1] == '.'):\n",
    "        return(tagged[0])\n",
    "    else:\n",
    "        return('{}_{}'.format(tagged[0], tagged[1]))\n",
    "\n",
    "def apply_tags(list_of_sentences):\n",
    "    tagged = []\n",
    "    for sentence in list_of_sentences:\n",
    "        tokenized = nltk.word_tokenize(sentence)\n",
    "        with_pos_tags = [merge_token_pos(tag) for tag in nltk.pos_tag(tokenized)]\n",
    "        tagged.append(\" \".join(with_pos_tags))\n",
    "    return(tagged)\n",
    "        \n",
    "yelp_x_tagged = apply_tags(yelp_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-tagged: 32632\tTagged: 32632\n"
     ]
    }
   ],
   "source": [
    "# Should be the same size\n",
    "print('Non-tagged: {}\\tTagged: {}'.format(len(yelp_x), len(yelp_x_tagged)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Yuck_NN ! This_DT place_NN needs_VBZ to_TO blown_VB up_RB and_CC rebuilt_VB . Cigarette_NNP smoke_NN and_CC dirty_NN chairs_NNS throughout_IN the_DT casino_NN . They_PRP usually_RB only_RB have_VBP one_CD bar_NN open_JJ with_IN one_CD bartender_NN so_RB expect_JJ to_TO wait_VB in_IN line_NN ! So_RB many_JJ nice_JJ casino_NN 's_POS in_IN the_DT valley_NN ,_, do_VBP n't_RB bother_VB with_IN the_DT Fort_NNP\",\n",
       " 'Yorkdale_NN is_VBZ cleaner_JJR than_IN most_JJS shopping_NN malls_NNS and_CC is_VBZ definitely_RB stepping_VBG up_RP and_CC improving_VBG every_DT time_NN I_PRP go_VBP there_RB . It_PRP has_VBZ expanded_VBN quite_RB significantly_RB with_IN new_JJ store_NN additions_NNS as_RB well_RB as_IN parking_VBG lots_NNS . During_IN holidays_NNS it_PRP can_MD definitely_RB get_VB crowded_VBN so_RB I_PRP highly_RB suggest_VBP to_TO shop_VB as_RB early_RB as_IN possible_JJ . As_IN for_IN the_DT food_NN court_NN ,_, the_DT have_VBP change_NN a_DT few_JJ and_CC provides_VBZ a_DT variety_NN of_IN options_NNS from_IN Indian_JJ to_TO Greek_VB to_TO japanese_JJ . They_PRP even_RB have_VBP a_DT couple_NN of_IN dine-in_JJ restaurants_NNS . The_DT franchise_NN pretty_RB much_JJ taste_NN similar_JJ to_TO the_DT other_JJ franchise_NN however_RB it_PRP is_VBZ definitely_RB cleaner_JJR than_IN most_JJS foodcourts_NNS .',\n",
       " \"We_PRP listened_VBD to_TO the_DT suggestions_NNS on_IN Yelp_NN and_CC were_VBD not_RB disappointed_VBN . My_PRP$ friend_NN and_CC I_PRP ordered_VBD the_DT Kim_NNP Chi_NNP fries_NNS ,_, the_DT five_CD spice_NN pork_NN belly_RB sandwhich_JJ and_CC the_DT pork_NN belly_RB bao_JJ . All_DT 3_CD were_VBD delicious_JJ and_CC packed_VBN with_IN flavours_NNS . The_DT Kim_NNP Chi_NNP fries_NNS is_VBZ a_DT combination_NN that_WDT works_VBZ surprisingly_RB well_RB . If_IN you_PRP are_VBP someone_NN that_WDT is_VBZ n't_RB too_RB adventurous_JJ ,_, you_PRP should_MD still_RB try_VB them_PRP . Very_RB good_JJ food_NN for_IN a_DT very_RB good_JJ price_NN\",\n",
       " \"My_PRP$ doctor_NN ,_, after_IN setting_VBG on_IN my_PRP$ mask_NN order_NN for_IN 8_CD days_NNS ,_, sent_VBD the_DT order_NN to_TO Lifecare_VB on_IN Thursday_NNP . I_PRP live_VBP in_IN a_DT gated_JJ community_NN . They_PRP tried_VBD to_TO deliver_VB the_DT next_JJ day_NN on_IN Friday_NNP when_WRB I_PRP was_VBD n't_RB home_NN and_CC did_VBD n't_RB think_VB to_TO call_VB in_IN advance_NN to_TO schedule_VB delivery_NN . I_PRP called_VBD Saturday_NNP morning_NN and_CC was_VBD told_VBN someone_NN would_MD call_VB me_PRP back_RB . No_CC one_CD did_VBD . I_PRP called_VBD back_RB at_IN 4:00_CD and_CC was_VBD told_VBN no_DT one_NN was_VBD available_JJ to_TO deliver_VB the_DT mask_NN that_DT day_NN so_IN it_PRP would_MD be_VB Monday_NNP . I_PRP called_VBD Monday_NNP and_CC was_VBD told_VBN that_IN it_PRP would_MD be_VB Tuesday_NNP or_CC Wednesday_NNP . I_PRP told_VBD them_PRP this_DT was_VBD unacceptable_JJ since_IN I_PRP had_VBD been_VBN without_IN my_PRP$ CPAP_NN for_IN almost_RB 2_CD weeks_NNS . I_PRP gave_VBD them_PRP the_DT gate_NN code_NN and_CC they_PRP promised_VBD to_TO deliver_VB the_DT mask_NN today_NN . The_DT person_NN then_RB asked_VBD what_WP type_NN of_IN mask_NN I_PRP had_VBD . I_PRP could_MD n't_RB believe_VB what_WP I_PRP just_RB heard_RB . They_PRP knew_VBD exactly_RB what_WP I_PRP had_VBD but_CC their_PRP$ internal_JJ communications_NNS are_VBP awful_JJ . I_PRP am_VBP still_RB waiting_VBG for_IN my_PRP$ mask_NN . This_DT people_NNS are_VBP totally_RB incompetent_JJ .\",\n",
       " \"I_PRP think_VBP it_PRP says_VBZ a_DT lot_NN that_IN I_PRP work_VBP 2_CD minutes_NNS from_IN this_DT DD_NNP and_CC I_PRP only_RB go_VBP there_RB when_WRB I_PRP absolutely_RB need_VBP to_TO . Parking_VBG and_CC entrance/exit_JJ situation_NN is_VBZ awful_JJ ,_, and_CC any_DT food_NN or_CC coffee_NN I_PRP 've_VBP received_VBN there_RB has_VBZ n't_RB been_VBN amazing_VBG . Yesterday_NN ,_, I_PRP just_RB got_VBD a_DT simple_JJ coffee_NN with_IN almond_JJ milk_NN and_CC it_PRP had_VBD n't_RB been_VBN stirred/coffee_VBN was_VBD pretty_RB weak_JJ . I_PRP think_VBP you_PRP get_VBP what_WP you_PRP pay_VBP for_IN ,_, and_CC while_IN DD_NNP is_VBZ cheaper_JJR than_IN Starbucks_NNS ,_, this_DT particular_JJ location_NN just_RB does_VBZ n't_RB do_VB a_DT good_JJ enough_RB job_NN to_TO make_VB me_PRP want_VB to_TO switch_VB .\"]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_x_tagged[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = t.texts_to_sequences(yelp_x)\n",
    "X = sequence.pad_sequences(X, maxlen=max_length, padding='post')\n",
    "\n",
    "# Define split\n",
    "split = round(len(X) *.8)\n",
    "split\n",
    "\n",
    "# Actually split the data\n",
    "X_train = X[:split - 1]\n",
    "X_test = X[split:]\n",
    "Y_train = yelp_y[:split -1]\n",
    "Y_test = yelp_y[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 1005, 300)         3000000   \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 1005, 64)          57664     \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 1005, 32)          6176      \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 1005, 16)          1552      \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 16080)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 16080)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 180)               2894580   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 180)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 181       \n",
      "=================================================================\n",
      "Total params: 5,960,153\n",
      "Trainable params: 5,960,153\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Using embedding from Keras\n",
    "embedding_vecor_length = 300\n",
    "model_tags = Sequential()\n",
    "model_tags.add(Embedding(top_words, embedding_vecor_length, input_length=max_length))\n",
    "\n",
    "# Convolutional model (3x conv, flatten, 2x dense)\n",
    "model_tags.add(Convolution1D(64, 3, padding='same'))\n",
    "model_tags.add(Convolution1D(32, 3, padding='same'))\n",
    "model_tags.add(Convolution1D(16, 3, padding='same'))\n",
    "model_tags.add(Flatten())\n",
    "model_tags.add(Dropout(0.2))\n",
    "model_tags.add(Dense(180,activation='sigmoid'))\n",
    "model_tags.add(Dropout(0.2))\n",
    "model_tags.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "# Log to tensorboard\n",
    "tensorBoardCallback = TensorBoard(log_dir='./logs', write_graph=True)\n",
    "model_tags.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model_tags.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "26105/26105 [==============================] - 264s 10ms/step - loss: 0.6637 - acc: 0.6234\n",
      "Epoch 2/3\n",
      "26105/26105 [==============================] - 203s 8ms/step - loss: 0.5115 - acc: 0.7501\n",
      "Epoch 3/3\n",
      "26105/26105 [==============================] - 808s 31ms/step - loss: 0.2014 - acc: 0.9186\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1489f0c88>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_tags.fit(X_train, Y_train, epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 58.75%\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of the model\n",
    "scores = model_tags.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
